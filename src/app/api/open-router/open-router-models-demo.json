{
  "data": [
    {
      "architecture": {
        "input_modalities": ["image", "text"],
        "instruct_type": null,
        "modality": "text+image->text",
        "output_modalities": ["text"],
        "tokenizer": "Claude"
      },
      "context_length": 200000,
      "created": 1729555200,
      "description": "New Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Scores ~49% on SWE-Bench Verified, higher than the last best score, and without any fancy prompt scaffolding\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\n#multimodal",
      "hugging_face_id": null,
      "id": "anthropic/claude-3.5-sonnet",
      "name": "Anthropic: Claude 3.5 Sonnet",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.000015",
        "image": "0.0048",
        "input_cache_read": "0.0000003",
        "input_cache_write": "0.00000375",
        "internal_reasoning": "0",
        "prompt": "0.000003",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "max_tokens",
        "stop",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "top_provider": {
        "context_length": 200000,
        "is_moderated": true,
        "max_completion_tokens": 8192
      }
    },
    {
      "architecture": {
        "input_modalities": ["image", "text"],
        "instruct_type": null,
        "modality": "text+image->text",
        "output_modalities": ["text"],
        "tokenizer": "Claude"
      },
      "context_length": 200000,
      "created": 1718841600,
      "description": "Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Autonomously writes, edits, and runs code with reasoning and troubleshooting\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\nFor the latest version (2024-10-23), check out [Claude 3.5 Sonnet](/anthropic/claude-3.5-sonnet).\n\n#multimodal",
      "hugging_face_id": null,
      "id": "anthropic/claude-3.5-sonnet-20240620:beta",
      "name": "Anthropic: Claude 3.5 Sonnet (2024-06-20) (self-moderated)",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.000015",
        "image": "0.0048",
        "input_cache_read": "0.0000003",
        "input_cache_write": "0.00000375",
        "internal_reasoning": "0",
        "prompt": "0.000003",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "max_tokens",
        "stop",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 8192
      }
    },
    {
      "architecture": {
        "input_modalities": ["image", "text"],
        "instruct_type": null,
        "modality": "text+image->text",
        "output_modalities": ["text"],
        "tokenizer": "Claude"
      },
      "context_length": 200000,
      "created": 1740422110,
      "description": "Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)",
      "hugging_face_id": "",
      "id": "anthropic/claude-3.7-sonnet",
      "name": "Anthropic: Claude 3.7 Sonnet",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.000015",
        "image": "0.0048",
        "input_cache_read": "0.0000003",
        "input_cache_write": "0.00000375",
        "internal_reasoning": "0",
        "prompt": "0.000003",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "stop",
        "temperature",
        "tool_choice",
        "tools",
        "top_k",
        "top_p"
      ],
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 64000
      }
    },
    {
      "architecture": {
        "input_modalities": ["image", "text"],
        "instruct_type": null,
        "modality": "text+image->text",
        "output_modalities": ["text"],
        "tokenizer": "Claude"
      },
      "context_length": 200000,
      "created": 1740422110,
      "description": "Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)",
      "hugging_face_id": "",
      "id": "anthropic/claude-3.7-sonnet:thinking",
      "name": "Anthropic: Claude 3.7 Sonnet (thinking)",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.000015",
        "image": "0.0048",
        "input_cache_read": "0.0000003",
        "input_cache_write": "0.00000375",
        "internal_reasoning": "0",
        "prompt": "0.000003",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "include_reasoning",
        "max_tokens",
        "reasoning",
        "stop",
        "temperature",
        "tool_choice",
        "tools"
      ],
      "top_provider": {
        "context_length": 200000,
        "is_moderated": true,
        "max_completion_tokens": 128000
      }
    },
    {
      "architecture": {
        "input_modalities": ["file", "image", "text"],
        "instruct_type": null,
        "modality": "text+image->text",
        "output_modalities": ["text"],
        "tokenizer": "Gemini"
      },
      "context_length": 1048576,
      "created": 1744914667,
      "description": "Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nNote: This model is available in two variants: thinking and non-thinking. The output pricing varies significantly depending on whether the thinking capability is active. If you select the standard variant (without the \":thinking\" suffix), the model will explicitly avoid generating thinking tokens. \n\nTo utilize the thinking capability and receive thinking tokens, you must choose the \":thinking\" variant, which will then incur the higher thinking-output pricing. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
      "hugging_face_id": "",
      "id": "google/gemini-2.5-flash-preview:thinking",
      "name": "Google: Gemini 2.5 Flash Preview 04-17 (thinking)",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.0000035",
        "image": "0.0006192",
        "input_cache_read": "0.0000000375",
        "input_cache_write": "0.0000002333",
        "internal_reasoning": "0",
        "prompt": "0.00000015",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "max_tokens",
        "response_format",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_p"
      ],
      "top_provider": {
        "context_length": 1048576,
        "is_moderated": false,
        "max_completion_tokens": 65535
      }
    },
    {
      "architecture": {
        "input_modalities": ["file", "image", "text"],
        "instruct_type": null,
        "modality": "text+image->text",
        "output_modalities": ["text"],
        "tokenizer": "Gemini"
      },
      "context_length": 1048576,
      "created": 1746578513,
      "description": "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.",
      "hugging_face_id": "",
      "id": "google/gemini-2.5-pro-preview",
      "name": "Google: Gemini 2.5 Pro Preview",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.00001",
        "image": "0.00516",
        "input_cache_read": "0.00000031",
        "input_cache_write": "0.000001625",
        "internal_reasoning": "0",
        "prompt": "0.00000125",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "max_tokens",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "tool_choice",
        "tools",
        "top_p"
      ],
      "top_provider": {
        "context_length": 1048576,
        "is_moderated": false,
        "max_completion_tokens": 65535
      }
    },
    {
      "architecture": {
        "input_modalities": ["text"],
        "instruct_type": null,
        "modality": "text->text",
        "output_modalities": ["text"],
        "tokenizer": "Other"
      },
      "context_length": 32768,
      "created": 1746130961,
      "description": "Phi-4-reasoning-plus is an enhanced 14B parameter model from Microsoft, fine-tuned from Phi-4 with additional reinforcement learning to boost accuracy on math, science, and code reasoning tasks. It uses the same dense decoder-only transformer architecture as Phi-4, but generates longer, more comprehensive outputs structured into a step-by-step reasoning trace and final answer.\n\nWhile it offers improved benchmark scores over Phi-4-reasoning across tasks like AIME, OmniMath, and HumanEvalPlus, its responses are typically ~50% longer, resulting in higher latency. Designed for English-only applications, it is well-suited for structured reasoning workflows where output quality takes priority over response speed.",
      "hugging_face_id": "microsoft/Phi-4-reasoning-plus",
      "id": "microsoft/phi-4-reasoning-plus",
      "name": "Microsoft: Phi 4 Reasoning Plus",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.00000035",
        "image": "0",
        "internal_reasoning": "0",
        "prompt": "0.00000007",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ],
      "top_provider": {
        "context_length": 32768,
        "is_moderated": false,
        "max_completion_tokens": null
      }
    },
    {
      "architecture": {
        "input_modalities": ["text"],
        "instruct_type": null,
        "modality": "text->text",
        "output_modalities": ["text"],
        "tokenizer": "Other"
      },
      "context_length": 32768,
      "created": 1746130961,
      "description": "Phi-4-reasoning-plus is an enhanced 14B parameter model from Microsoft, fine-tuned from Phi-4 with additional reinforcement learning to boost accuracy on math, science, and code reasoning tasks. It uses the same dense decoder-only transformer architecture as Phi-4, but generates longer, more comprehensive outputs structured into a step-by-step reasoning trace and final answer.\n\nWhile it offers improved benchmark scores over Phi-4-reasoning across tasks like AIME, OmniMath, and HumanEvalPlus, its responses are typically ~50% longer, resulting in higher latency. Designed for English-only applications, it is well-suited for structured reasoning workflows where output quality takes priority over response speed.",
      "hugging_face_id": "microsoft/Phi-4-reasoning-plus",
      "id": "microsoft/phi-4-reasoning-plus:free",
      "name": "Microsoft: Phi 4 Reasoning Plus (free)",
      "per_request_limits": null,
      "pricing": {
        "completion": "0",
        "image": "0",
        "internal_reasoning": "0",
        "prompt": "0",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "frequency_penalty",
        "include_reasoning",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "reasoning",
        "repetition_penalty",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "top_provider": {
        "context_length": 32768,
        "is_moderated": false,
        "max_completion_tokens": null
      }
    },
    {
      "architecture": {
        "input_modalities": ["text"],
        "instruct_type": null,
        "modality": "text->text",
        "output_modalities": ["text"],
        "tokenizer": "Other"
      },
      "context_length": 131072,
      "created": 1744119494,
      "description": "Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) optimized for advanced reasoning, conversational interactions, retrieval-augmented generation (RAG), and tool-calling tasks. Derived from Meta's Llama-3.3-70B-Instruct, it employs a Neural Architecture Search (NAS) approach, significantly enhancing efficiency and reducing memory requirements. This allows the model to support a context length of up to 128K tokens and fit efficiently on single high-performance GPUs, such as NVIDIA H200.\n\nNote: you must include `detailed thinking on` in the system prompt to enable reasoning. Please see [Usage Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) for more.",
      "hugging_face_id": "nvidia/Llama-3_3-Nemotron-Super-49B-v1",
      "id": "nvidia/llama-3.3-nemotron-super-49b-v1",
      "name": "NVIDIA: Llama 3.3 Nemotron Super 49B v1",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.0000004",
        "image": "0",
        "internal_reasoning": "0",
        "prompt": "0.00000013",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "top_provider": {
        "context_length": 131072,
        "is_moderated": false,
        "max_completion_tokens": null
      }
    },
    {
      "architecture": {
        "input_modalities": ["text"],
        "instruct_type": null,
        "modality": "text->text",
        "output_modalities": ["text"],
        "tokenizer": "Other"
      },
      "context_length": 131072,
      "created": 1744119494,
      "description": "Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) optimized for advanced reasoning, conversational interactions, retrieval-augmented generation (RAG), and tool-calling tasks. Derived from Meta's Llama-3.3-70B-Instruct, it employs a Neural Architecture Search (NAS) approach, significantly enhancing efficiency and reducing memory requirements. This allows the model to support a context length of up to 128K tokens and fit efficiently on single high-performance GPUs, such as NVIDIA H200.\n\nNote: you must include `detailed thinking on` in the system prompt to enable reasoning. Please see [Usage Recommendations](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1#quick-start-and-usage-recommendations) for more.",
      "hugging_face_id": "nvidia/Llama-3_3-Nemotron-Super-49B-v1",
      "id": "nvidia/llama-3.3-nemotron-super-49b-v1:free",
      "name": "NVIDIA: Llama 3.3 Nemotron Super 49B v1 (free)",
      "per_request_limits": null,
      "pricing": {
        "completion": "0",
        "image": "0",
        "internal_reasoning": "0",
        "prompt": "0",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_logprobs",
        "top_p"
      ],
      "top_provider": {
        "context_length": 131072,
        "is_moderated": false,
        "max_completion_tokens": null
      }
    },
    {
      "architecture": {
        "input_modalities": ["text"],
        "instruct_type": null,
        "modality": "text->text",
        "output_modalities": ["text"],
        "tokenizer": "GPT"
      },
      "context_length": 128000,
      "created": 1726099200,
      "description": "The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.",
      "hugging_face_id": null,
      "id": "openai/o1-preview-2024-09-12",
      "name": "OpenAI: o1-preview (2024-09-12)",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.00006",
        "image": "0",
        "input_cache_read": "0.0000075",
        "internal_reasoning": "0",
        "prompt": "0.000015",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": ["max_tokens", "seed"],
      "top_provider": {
        "context_length": 128000,
        "is_moderated": true,
        "max_completion_tokens": 32768
      }
    },
    {
      "architecture": {
        "input_modalities": ["file", "image", "text"],
        "instruct_type": null,
        "modality": "text+image->text",
        "output_modalities": ["text"],
        "tokenizer": "Other"
      },
      "context_length": 200000,
      "created": 1744823457,
      "description": "o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images. Note that BYOK is required for this model. Set up here: https://openrouter.ai/settings/integrations",
      "hugging_face_id": "",
      "id": "openai/o3",
      "name": "OpenAI: o3",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.00004",
        "image": "0.00765",
        "input_cache_read": "0.0000025",
        "internal_reasoning": "0",
        "prompt": "0.00001",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "max_tokens",
        "response_format",
        "seed",
        "structured_outputs",
        "tool_choice",
        "tools"
      ],
      "top_provider": {
        "context_length": 200000,
        "is_moderated": true,
        "max_completion_tokens": 100000
      }
    },
    {
      "architecture": {
        "input_modalities": ["image", "text"],
        "instruct_type": null,
        "modality": "text+image->text",
        "output_modalities": ["text"],
        "tokenizer": "Other"
      },
      "context_length": 200000,
      "created": 1744820942,
      "description": "OpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains.\n\nDespite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.",
      "hugging_face_id": "",
      "id": "openai/o4-mini",
      "name": "OpenAI: o4 Mini",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.0000044",
        "image": "0.0008415",
        "input_cache_read": "0.000000275",
        "internal_reasoning": "0",
        "prompt": "0.0000011",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "max_tokens",
        "response_format",
        "seed",
        "structured_outputs",
        "tool_choice",
        "tools"
      ],
      "top_provider": {
        "context_length": 200000,
        "is_moderated": true,
        "max_completion_tokens": 100000
      }
    },
    {
      "architecture": {
        "input_modalities": ["file", "image", "text"],
        "instruct_type": null,
        "modality": "text+image->text",
        "output_modalities": ["text"],
        "tokenizer": "Other"
      },
      "context_length": 200000,
      "created": 1744824212,
      "description": "OpenAI o4-mini-high is the same model as [o4-mini](/openai/o4-mini) with reasoning_effort set to high. \n\nOpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains.\n\nDespite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.",
      "hugging_face_id": "",
      "id": "openai/o4-mini-high",
      "name": "OpenAI: o4 Mini High",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.0000044",
        "image": "0.0008415",
        "input_cache_read": "0.000000275",
        "internal_reasoning": "0",
        "prompt": "0.0000011",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "max_tokens",
        "response_format",
        "seed",
        "structured_outputs",
        "tool_choice",
        "tools"
      ],
      "top_provider": {
        "context_length": 200000,
        "is_moderated": true,
        "max_completion_tokens": 100000
      }
    },
    {
      "architecture": {
        "input_modalities": ["image", "text"],
        "instruct_type": null,
        "modality": "text+image->text",
        "output_modalities": ["text"],
        "tokenizer": "Grok"
      },
      "context_length": 32768,
      "created": 1734237338,
      "description": "Grok 2 Vision 1212 advances image-based AI with stronger visual comprehension, refined instruction-following, and multilingual support. From object recognition to style analysis, it empowers developers to build more intuitive, visually aware applications. Its enhanced steerability and reasoning establish a robust foundation for next-generation image solutions.\n\nTo read more about this model, check out [xAI's announcement](https://x.ai/blog/grok-1212).",
      "hugging_face_id": "",
      "id": "x-ai/grok-2-vision-1212",
      "name": "xAI: Grok 2 Vision 1212",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.00001",
        "image": "0.0036",
        "internal_reasoning": "0",
        "prompt": "0.000002",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "frequency_penalty",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_logprobs",
        "top_p"
      ],
      "top_provider": {
        "context_length": 32768,
        "is_moderated": false,
        "max_completion_tokens": null
      }
    },
    {
      "architecture": {
        "input_modalities": ["text"],
        "instruct_type": null,
        "modality": "text->text",
        "output_modalities": ["text"],
        "tokenizer": "Grok"
      },
      "context_length": 131072,
      "created": 1744240068,
      "description": "Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.\n\nExcels in structured tasks and benchmarks like GPQA, LCB, and MMLU-Pro where it outperforms Grok 3 Mini even on high thinking. \n\nNote: That there are two xAI endpoints for this model. By default when using this model we will always route you to the base endpoint. If you want the fast endpoint you can add `provider: { sort: throughput}`, to sort by throughput instead. \n",
      "hugging_face_id": "",
      "id": "x-ai/grok-3-beta",
      "name": "xAI: Grok 3 Beta",
      "per_request_limits": null,
      "pricing": {
        "completion": "0.000015",
        "image": "0",
        "internal_reasoning": "0",
        "prompt": "0.000003",
        "request": "0",
        "web_search": "0"
      },
      "supported_parameters": [
        "frequency_penalty",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "tool_choice",
        "tools",
        "top_logprobs",
        "top_p"
      ],
      "top_provider": {
        "context_length": 131072,
        "is_moderated": false,
        "max_completion_tokens": null
      }
    }
  ]
}
